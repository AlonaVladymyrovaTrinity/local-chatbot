# ğŸ¤– Local Chatbot App (React + Node.js + Ollama)

A private, local chatbot web app that runs entirely on your machine using open-source language models. Built with:

- **React + Tailwind CSS** (Frontend)
- **Node.js + Express** (Backend)
- **Ollama + Mistral** (AI Model - runs locally)

No cloud, no API keys, no data sharing â€” 100% private AI.

---

## ğŸš€ Features

- ğŸ” Real-time chat interface with a local AI model
- ğŸ¨ Clean, responsive UI built with Tailwind CSS
- ğŸ§  Model powered by Mistral via [Ollama](https://ollama.com)
- ğŸŒ Full-stack integration: React frontend + Node backend
- ğŸ’¬ Contextual responses (can be extended with memory)
- ğŸ–¥ï¸ Fully local & offline-ready

---

## ğŸ› ï¸ Tech Stack

| Layer      | Tech Used                |
|------------|--------------------------|
| Frontend   | React, Tailwind CSS      |
| Backend    | Node.js, Express         |
| AI Model   | Mistral via Ollama       |
| API Comm   | Native `fetch()`         |

---

## âš™ Setup Instructions

### 1. Clone the Repository

`git clone https://github.com/your-username/local-chatbot.git`
`cd local-chatbot`

### 2. Install and Run Ollama (AI Model)
Download Ollama:
ğŸ‘‰ https://ollama.com

Then, in your terminal:
# Download and run Mistral model
`ollama run mistral`

# OR start Ollama API server (recommended)
`ollama serve`

ğŸ“Œ Mistral will be downloaded automatically the first time.

### 3. Backend Setup
`cd backend`
`npm install`
`node index.js`

Backend will run at:
ğŸ‘‰ http://localhost:3001

### 4. Frontend Setup

`cd ../frontend`
`npm install`
`npm start`

Frontend will run at:
ğŸ‘‰ http://localhost:3000

### ğŸ“¦ Project Structure

```local-chatbot/
â”œâ”€â”€ backend/            # Node.js + Express API
â”‚   â””â”€â”€ index.js
â”œâ”€â”€ frontend/           # React app with Tailwind CSS
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ tailwind.config.js
â”œâ”€â”€ README.md
```

### ğŸ§ª Example Usage

Send a message in the chat UI like:

> "Tell me a joke"

You'll get a response from the AI model

### ğŸ’¡ Ideas for Future Improvements
- ğŸ§  Memory system for ongoing conversation context
- ğŸ§ Custom persona support (friendly, sarcastic, mentor)
- ğŸŒ“ Dark mode toggle
- ğŸ’¾ Save/load chat history locally
- ğŸ”€ Model selector (Mistral, LLaMA3, etc.)

### ğŸ§° Requirements
- Node.js v18.20.8 or higher (for native fetch)
- Ollama installed and running (ollama serve)
- Model installed: mistral (or your choice)

### ğŸ™Œ Acknowledgments
[Ollama](https://ollama.com/) â€“ for local model running
[Mistral](https://mistral.ai/) â€“ the conversational model
[Tailwind CSS](https://tailwindcss.com/) â€“ fast styling
[React](https://react.dev/) â€“ frontend framework

### ğŸ“„ License
This project is for ***personal/local use only***. Check individual model licenses (like Mistral or LLaMA) for usage terms.